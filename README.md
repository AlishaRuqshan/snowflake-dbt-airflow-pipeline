ğŸš€ End-to-End Cloud Data Pipeline Project (Snowflake + dbt + Apache Airflow)

**ğŸ“Œ Project Overview**
- 	ğŸ” **Created a secure environment in Snowflake:**
	 - â€¢Set up a dedicated database (dbt_db) and schema (dbt_schema)
	 - â€¢Created a custom role (dbt_role) and granted it permission to create tables, views, stages, and formats
	 - â€¢Assigned the role to your Snowflake user.
- ğŸ› ï¸** Built a DBT project locally:**
	 - â€¢Initialized a new DBT project (data_pipeline)
  - â€¢Configured profiles.yml with your Snowflake connection details (user, password, warehouse, schema)
	 - â€¢Defined staging and mart models using SQL files and config() blocks
	 - â€¢Wrote reusable macros for transformations.
- âš™ï¸ **Set up Airflow locally using Astro CLI:**
	 - â€¢Ran astro dev init to scaffold the Airflow project
	 - â€¢Wrote a DAG (dbt_dag.py) that uses BashOperator to trigger dbt run and dbt test
	 - â€¢Mounted the DBT project folder into the dags/ directory of the Astro environment
	 - â€¢Added the profiles.yml inside a hidden .dbt folder so Airflow could access it.
- **ğŸ” Ran and orchestrated everything locally:**
	 - â€¢Triggered the Airflow DAG manually via the UI
	 - â€¢Verified task status in the Graph view
	 - â€¢Checked logs to confirm DBT transformations executed successfully
	 - â€¢Models appeared in Snowflake as expected (with correct materializations: table, view)
 
**ğŸ§± Project Folder Breakdown**

dbt-dag/

â”œâ”€â”€ dags/

â”‚   â”œâ”€â”€ data_pipeline/       # Your dbt project: models, macros, seeds, snapshots

â”‚   â””â”€â”€ .dbt/profiles.yml    # Snowflake profile â€“ safely mounted inside container

â”œâ”€â”€ plugins/                 # Airflow plugins if needed

â”œâ”€â”€ Dockerfile               # Astro Runtime image (comes with dbt)

â”œâ”€â”€ requirements.txt         # Extra Python packages

â”œâ”€â”€ airflow_settings.yaml    # Airflow config: DAGs, connections, variables

â””â”€â”€ README.md



**ğŸ“ˆ What You'll See Working**

Airflow UI running locally at localhost:8080
dbt models successfully building tables and views in Snowflake
A clean, Git-tracked project you can share or extend


**ğŸ› ï¸ Getting Started Locally**

If you want to run this project on your machine:

ğŸ” Prerequisites:

Install Docker

Install Astro CLI

Clone this repo:

git clone https://github.com/AlishaRuqshan/snowflake-dbt-airflow-pipeline.git
cd snowflake-dbt-airflow-pipeline

ğŸš€ Start the local environment:

astro dev start --wait 3m

ğŸŒ Access Airflow:

Visit http://localhost:8080 and run the DAG named run_dbt_pipeline

ğŸ§ª Add your own Snowflake credentials inside dags/.dbt/profiles.yml

Or use env_var() style secrets with astro dev secrets set
