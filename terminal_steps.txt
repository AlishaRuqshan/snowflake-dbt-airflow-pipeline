# 1. Install virtual environment and activate
python3 -m venv env
source env/bin/activate

# 2. Install DBT and Snowflake adapter
pip install dbt-core dbt-snowflake

# 3. Initialize a new DBT project (if not already)
dbt init data_pipeline

# 4. Create your dbt_project.yml, models/, seeds/, macros/ etc. (Done manually or by cloning your repo)

# 5. Create Snowflake profile
mkdir -p ~/.dbt

# Create this file with credentials (via nano, vim, or VS Code)
nano ~/.dbt/profiles.yml
# (or use the env_var approach if pushing to GitHub)

# 6. Check dbt works
dbt debug

# 7. Move project files into Astro's DAGs folder
mv data_pipeline dags/data_pipeline

# 8. Initialize Astro project
astro dev init

# 9. Copy Snowflake profile into DAGs for container visibility
mkdir -p dags/.dbt
cp ~/.dbt/profiles.yml dags/.dbt/

# 10. Start Airflow container
astro dev start --wait 3m

# 11. Enter container (if needed for dbt CLI usage)
astro dev bash

# 12. Inside container: run dbt
dbt debug
dbt deps
dbt run
